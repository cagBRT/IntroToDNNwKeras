{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YODMye2bEgwF"
      },
      "source": [
        "# Deep Learning models using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVl4D6l7EgwK"
      },
      "source": [
        "Keras is a user-friendly neural network library written in Python. We will build a regression model to predict an employee's wage per hour.\n",
        "\n",
        "Note: The datasets we will be using are relatively clean, so we will not perform any data preprocessing in order to get our data ready for modeling. Datasets that you will use in future projects may not be so clean - for example, they may have missing values - so you may need to use data preprocessing techniques to alter your datasets to get more accurate results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5Jshat2EgwM"
      },
      "source": [
        "# Regression Model using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpzMj8y-EgwN"
      },
      "source": [
        "For our regression deep learning model, the first step is to read in the data we will use as input. For this example, we are using the 'hourly wages' dataset. To start, we will use Pandas to read in the data. I will not go into detail on Pandas, but it is a library you should become familiar with if you're looking to dive further into data science and machine learning.\n",
        "\n",
        "'df' stands for dataframe. Pandas read in the CSV file as a dataframe. The 'head()' function will show the first 5 rows of the dataframe so you can check that the data has been read in properly and can take an initial look at how the data is structured."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1rVIXYYEgwO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import to_categorical"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/cagBRT/IntroToDNNwKeras.git cloned-repo\n",
        "%cd cloned-repo"
      ],
      "metadata": {
        "id": "deDEcmR_HTAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOq82fE2EgwR"
      },
      "outputs": [],
      "source": [
        "#read in training data\n",
        "wage_df = pd.read_csv('/content/cloned-repo/hourly_wage')\n",
        "\n",
        "#view data structure\n",
        "wage_df.head()\n",
        "print(wage_df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDCeI1OnEgwS"
      },
      "source": [
        "Next, we need to split up our dataset into inputs (train_X) and our target (train_y). Our input will be every column except 'wage_per_hour' because 'wage_per_hour' is what we will be attempting to predict. Therefore, 'wage_per_hour' will be our target.\n",
        "\n",
        "We will use the pandas 'drop' function to drop the column 'wage_per_hour' from our dataframe and store it in the variable 'train_X'. This will be our input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjw70cmSEgwT"
      },
      "outputs": [],
      "source": [
        "#create a dataframe with all training data except the target column\n",
        "wage_data = wage_df.drop(columns=['wage_per_hour'])\n",
        "\n",
        "#check that the target variable has been removed\n",
        "wage_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PMskBQ4EgwU"
      },
      "outputs": [],
      "source": [
        "#create a label column with only the target column\n",
        "wage_y = wage_df[['wage_per_hour']]\n",
        "\n",
        "#view dataframe\n",
        "wage_y.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    wage_data, wage_y, test_size=0.20)"
      ],
      "metadata": {
        "id": "O2HInyIPkdnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z_Ifn10EgwV"
      },
      "source": [
        "The model type that we will be using is Sequential. Sequential is the easiest way to build a model in Keras. It allows you to build a model layer by layer. Each layer has weights that correspond to the layer the follows it.\n",
        "\n",
        "We use the 'add()' function to add layers to our model. We will add two layers and an output layer.\n",
        "\n",
        "'Dense' is the layer type. Dense is a standard layer type that works for most cases. In a dense layer, all nodes in the previous layer connect to the nodes in the current layer.\n",
        "\n",
        "We have 10 nodes in each of our input layers. This number can also be in the hundreds or thousands. Increasing the number of nodes in each layer increases model capacity. I will go into further detail about the effects of increasing model capacity shortly.\n",
        "\n",
        "'Activation' is the activation function for the layer. An activation function allows models to take into account nonlinear relationships. For example, if you are predicting diabetes in patients, going from age 10 to 11 is different than going from age 60Ã¢??61.\n",
        "\n",
        "The activation function we will be using is ReLU or Rectified Linear Activation. Although it is two linear pieces, it has been proven to work well in neural networks.\n",
        "\n",
        "The first layer needs an input shape. The input shape specifies the number of rows and columns in the input. The number of columns in our input is stored in 'n_cols'. There is nothing after the comma which indicates that there can be any amount of rows.\n",
        "\n",
        "The last layer is the output layer. It only has one node, which is for our prediction.\n",
        "\n",
        "Next, we need to compile our model. Compiling the model takes two parameters: optimizer and loss.\n",
        "\n",
        "The optimizer controls the learning rate. We will be using 'Adam' as our optimizer. Adam is generally a good optimizer to use for many cases. The Adam optimizer adjusts the learning rate throughout training.\n",
        "\n",
        "The learning rate determines how fast the optimal weights for the model are calculated. A smaller learning rate may lead to more accurate weights (up to a certain point), but the time it takes to compute the weights will be longer.\n",
        "\n",
        "For our loss function, we will use 'mean_squared_error'. It is calculated by taking the average squared difference between the predicted and actual values. It is a popular loss function for regression problems. The closer to 0 this is, the better the model performed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y7fzUJVEgwW"
      },
      "outputs": [],
      "source": [
        "#create model\n",
        "model = Sequential()\n",
        "\n",
        "#get number of columns in training data\n",
        "n_cols = X_train.shape[1]\n",
        "\n",
        "#add model layers\n",
        "model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "#compile model using mse as a measure of model performance\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=[\"mean_squared_error\"])\n",
        "\n",
        "#set early stopping monitor so the model stops training when it won't improve anymore\n",
        "early_stopping_monitor = EarlyStopping(patience=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS1b5W8zEgwX"
      },
      "source": [
        "Now we will train our model. To train, we will use the 'fit()' function on our model with the following five parameters: training data (train_X), target data (train_y), validation split, the number of epochs and callbacks.\n",
        "\n",
        "The validation split will randomly split the data into use for training and testing. During training, we will be able to see the validation loss, which gives the mean squared error of our model on the validation set. We will set the validation split at 0.2, which means that 20% of the training data we provide in the model will be set aside for testing model performance.\n",
        "\n",
        "The number of epochs is the number of times the model will cycle through the data. The more epochs we run, the more the model will improve, up to a certain point. After that point, the model will stop improving during each epoch. In addition, the more epochs, the longer the model will take to run. To monitor this, we will use 'early stopping'.\n",
        "\n",
        "Early stopping will stop the model from training before the number of epochs is reached if the model stops improving. We will set our early stopping monitor to 3. This means that after 3 epochs in a row in which the model doesn't improve, training will stop. Sometimes, the validation loss can stop improving then improve in the next epoch, but after 3 epochs in which the validation loss doesn't improve, it usually won't improve again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQNkvwR4EgwX"
      },
      "outputs": [],
      "source": [
        "#train model\n",
        "model.fit(X_train, y_train, validation_split=0.2, epochs=30, \n",
        "          callbacks=[early_stopping_monitor])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlpHoj5pEgwY"
      },
      "source": [
        "Making predictions on new data\n",
        "\n",
        "If you want to use this model to make predictions on new data, we would use the 'predict()' function, passing in our new data.\n",
        "The output would be 'wage_per_hour' predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq35ORSvEgwY"
      },
      "outputs": [],
      "source": [
        "wage_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_wages=y_test[[\"wage_per_hour\"]].values\n",
        "#y_test_wages"
      ],
      "metadata": {
        "id": "ndm2JK3qq_Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"prediction\", \"\\tground truth\")\n",
        "#for i in range(len(y_test_wages)):\n",
        "#  print(wage_pred[i],\"\\t\", y_test_wages[i])"
      ],
      "metadata": {
        "id": "EWu7LTNDl-0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "rms_dnn_1=np.sqrt(mean_squared_error(y_test, wage_pred))\n",
        "rms_dnn_1"
      ],
      "metadata": {
        "id": "41ymV67ovFyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWguv-zUEgwY"
      },
      "source": [
        "As you increase the number of nodes and layers in a model, the model capacity increases. Increasing model capacity can lead to a more accurate model, up to a certain point, at which the model will stop improving. Generally, the more training data you provide, the larger the model should be. We are only using a tiny amount of data, so our model is pretty small. The larger the model, the more computational capacity it requires and it will take longer to train.\n",
        "\n",
        "Let's create a new model using the same training data as our previous model. This time, we will add a layer and increase the nodes in each layer to 200. We will train the model to see if increasing the model capacity will improve our validation score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k49HBNnqEgwZ"
      },
      "outputs": [],
      "source": [
        "#training a new model on the same data to show the effect of increasing model capacity\n",
        "#create model\n",
        "model_mc = Sequential()\n",
        "\n",
        "#add model layers\n",
        "model_mc.add(Dense(200, activation='relu', input_shape=(n_cols,)))\n",
        "model_mc.add(Dense(200, activation='relu'))\n",
        "model_mc.add(Dense(200, activation='relu'))\n",
        "model_mc.add(Dense(1))\n",
        "\n",
        "#compile model using mse as a measure of model performance\n",
        "model_mc.compile(optimizer='adam', loss='mean_squared_error', metrics=[\"mean_squared_error\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBoBuQ_MEgwZ"
      },
      "outputs": [],
      "source": [
        "#train model\n",
        "model_mc.fit(X_train, y_train, validation_split=0.2, \n",
        "             epochs=30, callbacks=[early_stopping_monitor])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgDYOteHqEJ5"
      },
      "outputs": [],
      "source": [
        "wage_pred_mc = model_mc.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rms_dnn=np.sqrt(mean_squared_error(y_test, wage_pred_mc))\n",
        "rms_dnn"
      ],
      "metadata": {
        "id": "h4i7C9F7vW86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"prediction\", \"\\tground truth\")\n",
        "#for i in range(len(y_test_wages)):\n",
        "#  print(wage_pred_mc[i],\"\\t\", y_test_wages[i])\n"
      ],
      "metadata": {
        "id": "Hp_Ehhp5qLTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model_sklearn=LinearRegression().fit(X_train, y_train)\n",
        "pred_sklearn=model_sklearn.predict(X_test)"
      ],
      "metadata": {
        "id": "M8KVG9F4tL28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_sklearn.score(X_test, y_test)"
      ],
      "metadata": {
        "id": "c7Cg1fl5tdfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "rms_sklearn=np.sqrt(mean_squared_error(y_test, pred_sklearn))\n",
        "rms_sklearn"
      ],
      "metadata": {
        "id": "B8G4cF8Wt_SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"sklearn model:\", rms_sklearn) \n",
        "print(\"dnn model:\",rms_dnn)\n",
        "print( \"first dnn model:\", rms_dnn_1)"
      ],
      "metadata": {
        "id": "bt1XFXNAVWDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Qq4wrgxqidd-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAJ1QLkREgwa"
      },
      "source": [
        "# Classification Model using Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KmygUY4Egwa"
      },
      "source": [
        "Now let's move on to building our model for classification. Since many steps will be a repeat from the previous model, I will only go over new concepts."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "   Given is the attribute name, attribute type, the measurement unit and a\n",
        "   brief description.  The number of rings is the value to predict: either\n",
        "   as a continuous value or as a classification problem.\n",
        "\n",
        "\tName\t\tData Type\tMeas.\tDescription\n",
        "\t----\t\t---------\t-----\t-----------\n",
        "\tSex\t\tnominal\t\t\tM, F, and I (infant)\n",
        "\tLength\t\tcontinuous\tmm\tLongest shell measurement\n",
        "\tDiameter\tcontinuous\tmm\tperpendicular to length\n",
        "\tHeight\t\tcontinuous\tmm\twith meat in shell\n",
        "\tWhole weight\tcontinuous\tgrams\twhole abalone\n",
        "\tShucked weight\tcontinuous\tgrams\tweight of meat\n",
        "\tViscera weight\tcontinuous\tgrams\tgut weight (after bleeding)\n",
        "\tShell weight\tcontinuous\tgrams\tafter being dried\n",
        "\tRings\t\tinteger\t\t\t+1.5 gives the age in years\n"
      ],
      "metadata": {
        "id": "3XBa0PQyW00R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem."
      ],
      "metadata": {
        "id": "Fur_SncWWnNT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_3seot9Egwb"
      },
      "outputs": [],
      "source": [
        "#read in training data\n",
        "train_df_2 = pd.read_csv('/content/abalone.data')\n",
        "\n",
        "#view data structure\n",
        "train_df_2.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_df_2['M'] = train_df_2.M.astype('category')\n",
        "#train_df_2[\"M\"]\n",
        "train_df_2=train_df_2.drop(columns=['M'])"
      ],
      "metadata": {
        "id": "BB0me0FzntrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols=train_df_2.columns\n",
        "train_df_2 = train_df_2.astype({'0.455':'float', '0.365':'float', '0.095':'float', '0.514':'float', '0.2245':'float', '0.101':'float', '0.15':'float'})"
      ],
      "metadata": {
        "id": "P1bMoHKgnQs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs1unjgWEgwb"
      },
      "outputs": [],
      "source": [
        "#create a dataframe with all training data except the target column\n",
        "train_X_2 = train_df_2.drop(columns=['15'])\n",
        "train_y_2 = train_df_2[[\"15\"]]\n",
        "#check that the target variable has been removed\n",
        "train_X_2.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_y_2"
      ],
      "metadata": {
        "id": "5LMq7JmaYQmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eu1W0mwEgwc"
      },
      "source": [
        "When separating the target column, we need to call the 'to_categorical()' function so that column will be 'one-hot encoded'. Currently, a patient with no diabetes is represented with a 0 in the diabetes column and a patient with diabetes is represented with a 1. \n",
        "\n",
        "With one-hot encoding, the integer will be removed and a binary variable is inputted for each category. In our case, we have two categories: no diabetes and diabetes.\n",
        "\n",
        "A patient with no diabetes will be represented by [1 0] and a patient with diabetes will be represented by [0 1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1cfOE67Egwd"
      },
      "outputs": [],
      "source": [
        "#one-hot encode target column\n",
        "train_y_2 = to_categorical(train_y_2,30)\n",
        "\n",
        "#vcheck that target column has been converted\n",
        "train_y_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EG_hAg2Egwe"
      },
      "source": [
        "The last layer of our model has 2 nodes - one for each option: the patient has diabetes or they don't.\n",
        "\n",
        "The activation is 'softmax'. Softmax makes the output sum up to 1 so the output can be interpreted as probabilities. The model will then make its prediction based on which option has a higher probability.\n",
        "\n",
        "We will use 'categorical_crossentropy' for our loss function. This is the most common choice for classification. A lower score indicates that the model is performing better.\n",
        "\n",
        "To make things even easier to interpret, we will use the 'accuracy' metric to see the accuracy score on the validation set at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(train_X_2.shape)\n",
        "outputs=train_y_2.shape\n",
        "print(outputs)"
      ],
      "metadata": {
        "id": "7S-ddP84ZSeq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwHNlX7WEgwf"
      },
      "outputs": [],
      "source": [
        "#create model\n",
        "model_2 = Sequential()\n",
        "#get number of columns in training data\n",
        "inputs=train_X_2.shape[1]\n",
        "#add layers to model\n",
        "model_2.add(Dense(250, activation='relu', input_shape=(inputs,)))\n",
        "model_2.add(Dense(250, activation='relu'))\n",
        "model_2.add(Dense(250, activation='relu'))\n",
        "model_2.add(Dense(250, activation='relu'))\n",
        "model_2.add(Dense(30, activation='softmax'))\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#compile model using accuracy to measure model performance\n",
        "model_2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "V-w9_ZfqZCDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y = np.asarray(train_y_2).astype(np.int32)\n",
        "print(y[1])"
      ],
      "metadata": {
        "id": "JbxZFTSMfGFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_X_2 = np.array([np.array(val) for val in train_X_2])"
      ],
      "metadata": {
        "id": "L8pkXD_6mQC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6CaY2OAEgwf"
      },
      "outputs": [],
      "source": [
        "#train model\n",
        "model_2.fit(train_X_2, train_y_2, epochs=120)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}