{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO0hulshb/jllLziSkXpOmB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/IntroToDNNwKeras/blob/master/Train_Test_sets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we look at doing a train-test-validate split.<br>\n",
        "\n",
        "We also look at linear and polynomial regression and using cross validation"
      ],
      "metadata": {
        "id": "GtIeM1gbW5Gj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example of Dataset split and cross-validation**"
      ],
      "metadata": {
        "id": "8N40cbrQGzNv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXqYak4-9E6u"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Assuming we have a dataset `X` of features and `y` of corresponding labels\n",
        "# Splitting the data into training and test sets\n",
        "# Generate some random data\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1)\n",
        "y = 2 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
        "                                                    random_state=42)\n",
        "# Splitting the training set into training and validation sets\n",
        "\n",
        "\n",
        "\n",
        "print(\"X_train: \",X_train.shape)\n",
        "print(\"y_train: \",y_train.shape)\n",
        "print(\"X_test: \",X_test.shape)\n",
        "print(\"y_test: \",y_test.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "  X_train,\n",
        "  y_train,\n",
        "  test_size=0.2,\n",
        "  random_state=42)\n",
        "\n",
        "print(\"X_train: \",X_train.shape)\n",
        "print(\"y_train: \",y_train.shape)\n",
        "print(\"X_val: \",X_val.shape)\n",
        "print(\"y_val: \",y_val.shape)"
      ],
      "metadata": {
        "id": "jpL7IWfI-HXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "00Ha8ogpOdf7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An example of using the training and test sets with a linear reg and poly reg models"
      ],
      "metadata": {
        "id": "uJl8wLwLG_0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_validate, train_test_split\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import sklearn\n",
        "\n",
        "np.random.seed(42)"
      ],
      "metadata": {
        "id": "R7LsBnGcHB7R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate and plot the dataset**"
      ],
      "metadata": {
        "id": "_UAnHweNHXTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate data and plot\n",
        "N = 300\n",
        "x = np.linspace(0, 7*np.pi, N)\n",
        "smooth = 1 + 0.5*np.sin(x)\n",
        "y = smooth + 0.2*np.random.randn(N)\n",
        "plt.plot(x, y)\n",
        "plt.plot(x, smooth)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.ylim(0,2)\n",
        "plt.legend([\"dataset\",\"noisy set\"], loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rfvmv9YTHGTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train-test split, intentionally use shuffle=False**<br>"
      ],
      "metadata": {
        "id": "u4gqcK0RHkbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**shuffle: bool, default=True**<br>\n",
        "Whether or not to shuffle the data before splitting.\n",
        "\n",
        "If shuffle=False then stratify must be None.<br>\n",
        "\n",
        "If not None, data is split in a stratified fashion, using this as the class labels.<br>\n",
        "<br>\n",
        "Stratified sampling is a technique that can help you improve the quality and efficiency of your machine learning models. It involves dividing your data into subgroups that share similar characteristics and then selecting a representative sample from each subgroup.<br>\n",
        "\n",
        "If the data ordering is not arbitrary (e.g. samples with the same class label are contiguous), ***shuffling the data first may be essential to get a meaningful cross- validation result***. <br>\n",
        "\n",
        "***However, the opposite may be true if the samples are not independently and identically distributed***. <br>\n",
        "\n",
        "For example, if samples correspond to news articles, and are ordered by their time of publication, then shuffling the data will likely lead to a model that is overfit and an inflated validation score: it will be tested on samples that are artificially similar (close in time) to training samples."
      ],
      "metadata": {
        "id": "4RTzpn2-UlNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train-test split, intentionally use shuffle=False\n",
        "X = x.reshape(-1,1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=False)\n",
        "print(\"X_train \", X_train.shape)\n",
        "print(\"X_test \", X_test.shape)"
      ],
      "metadata": {
        "id": "_V1zDKOWHkkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create two models: Polynomial and linear regression**"
      ],
      "metadata": {
        "id": "heg7W4vfIC24"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "degree = 2 #other degree value will create better model"
      ],
      "metadata": {
        "id": "6mFOPVtTW0qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "polyreg = make_pipeline(PolynomialFeatures(degree),\n",
        "                        LinearRegression(fit_intercept=False))\n",
        "linreg = LinearRegression()"
      ],
      "metadata": {
        "id": "VhMi56cCICDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use the training set to do cross-validation**"
      ],
      "metadata": {
        "id": "VeFADvBLIGDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-validation\n",
        "scoring = \"neg_root_mean_squared_error\"\n",
        "polyscores = cross_validate(polyreg, X_train, y_train, scoring=scoring,\n",
        "                            return_estimator=True)\n",
        "linscores = cross_validate(linreg, X_train, y_train, scoring=scoring,\n",
        "                           return_estimator=True)"
      ],
      "metadata": {
        "id": "RECucGYpIGMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compare the scores for the models**"
      ],
      "metadata": {
        "id": "Gw4xqy2lJFxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Which one is better? Linear and polynomial\n",
        "print(\"Linear regression score:\", linscores[\"test_score\"].mean())\n",
        "print(\"Polynomial regression score:\", polyscores[\"test_score\"].mean())\n",
        "print(\"Difference:\", linscores[\"test_score\"].mean() - polyscores[\"test_score\"].mean())"
      ],
      "metadata": {
        "id": "LggtjntvJF9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get the coefficients of the two regression models**"
      ],
      "metadata": {
        "id": "5rdZyoIwKDiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Coefficients of \")\n",
        "# Let's show the coefficient of the last fitted polynomial regression\n",
        "# This starts from the constant term and in ascending order of powers\n",
        "print(\"Polyreg: \",polyscores[\"estimator\"][0].steps[1][1].coef_)\n",
        "# And show the coefficient of the last-fitted linear regression\n",
        "print(\"Linearreg: \",linscores[\"estimator\"][0].intercept_,\n",
        "      linscores[\"estimator\"][-1].coef_)"
      ],
      "metadata": {
        "id": "lWaEDETJKDrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Polynomial Regression**:<br>\n",
        "y=1.2 -0.05x + 0.002x^2"
      ],
      "metadata": {
        "id": "QZ3cK3R4SbCy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear Regression:**<br>\n",
        "y=0.99x-0.001x"
      ],
      "metadata": {
        "id": "3YXN8fzjSyXI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot the models**"
      ],
      "metadata": {
        "id": "y_jVllxlKJxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot and compare\n",
        "plt.plot(x, y)#blue\n",
        "plt.plot(x, smooth)#orange\n",
        "plt.plot(x, polyscores[\"estimator\"][0].predict(X))#green\n",
        "plt.plot(x, linscores[\"estimator\"][0].predict(X))#red\n",
        "plt.ylim(0,2)\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.legend([\"dataset\",\"noisy set\",\"polyreg\", \"linearreg\"], loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sA4cjG_FKJ5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model and test it with the test set**"
      ],
      "metadata": {
        "id": "A8IuAOS_T1fN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we decided to use linear model for regression, we need to re-train the model and test it using our held out test data.<br>\n",
        "\n",
        "Cloning constructs a new unfitted estimator with the same parameters.\n",
        "<br>\n",
        "Clone does a deep copy of the model in an estimator without actually copying attached data. <br>\n",
        "\n",
        "It returns a new estimator with the same parameters that has not been fitted on any data."
      ],
      "metadata": {
        "id": "9NbxTwiqXOTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrain the model and evaluate\n",
        "linregclone = sklearn.base.clone(linreg)\n",
        "linregclone.fit(X_train, y_train)\n",
        "print(\"Test set RMSE:\", mean_squared_error(y_test, linregclone.predict(X_test), squared=False))\n",
        "print(\"Mean validation RMSE:\", -linscores[\"test_score\"].mean())"
      ],
      "metadata": {
        "id": "n2P9E2K0GqxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Assignment**<br>\n",
        "\n",
        "1. change shuffle in the train-test split to True.\n",
        "What happens to the models?\n",
        "\n",
        "2. Change the number of degrees of the polynomial regression model. Is there a number of degrees that is a better model?"
      ],
      "metadata": {
        "id": "MpUPOXBpWcA7"
      }
    }
  ]
}