{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOcFbxXO4t7v1hkypC2un/z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/IntroToDNNwKeras/blob/master/Underfit_and_Overfit_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qExt1h3z4B6"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import numpy as np\n",
        "from sklearn.model_selection import validation_curve\n",
        "from sklearn.datasets import load_iris\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "np.random.seed(0)\n",
        "#plt.style.use(‘ggplot’)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/is-your-model-overfitting-or-maybe-underfitting-an-example-using-a-neural-network-in-python-4faf155398d2"
      ],
      "metadata": {
        "id": "Gcb9Hmi404Re"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target"
      ],
      "metadata": {
        "id": "IXtqlZIRz-4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kf = KFold(n_splits=20)\n",
        "list_training_error = []\n",
        "list_testing_error = []\n",
        "for train_index, test_index in kf.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "    model = MLPRegressor()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_train_data_pred = model.predict(X_train)\n",
        "    y_test_data_pred = model.predict(X_test)\n",
        "    fold_training_error = mean_absolute_error(y_train, y_train_data_pred)\n",
        "    fold_testing_error = mean_absolute_error(y_test, y_test_data_pred)\n",
        "    list_training_error.append(fold_training_error)\n",
        "    list_testing_error.append(fold_testing_error)"
      ],
      "metadata": {
        "id": "HD5JA0EC0IOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1,2,1)\n",
        "plt.plot(range(1, kf.get_n_splits() + 1), np.array(list_training_error).ravel())\n",
        "plt.xlabel(\"number of fold\")\n",
        "plt.ylabel(\"training error\")\n",
        "plt.title(\"Training error across folds\")\n",
        "plt.tight_layout()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(range(1, kf.get_n_splits() + 1), np.array(list_testing_error).ravel())\n",
        "plt.xlabel(\"number of fold\")\n",
        "plt.ylabel(\"testing error\")\n",
        "plt.title(\"Testing error across folds\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3pINppbM0LJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s now interpret the results and try to spot overfitting and underfitting points.\n",
        "\n",
        "Reminder:\n",
        "\n",
        "Overfitting is when the model’s error on the training set (i.e. during training) is very low but then, the model’s error on the test set (i.e. unseen samples) is large!\n",
        "Underfitting is when the model’s error on both the training and test sets (i.e. during training and testing) is very high.\n",
        "An underfitting point can be identified at fold number 10. During the 10th fold, the error on the training set and test set is at the same time high! So the model underfits the portion of the training data that are passed in for training in fold 10.\n",
        "\n",
        "An overfitting point (not really extreme) can be seen at fold 20. The training MAE is around 0.16 and for the same fold the MAE on the test set is above 0.20.\n",
        "\n",
        "Disclaimer: These are not extreme cases of overfitting/underfitting but I just wanted to make my point and connect it with the theory I discussed in this article.\n",
        "\n"
      ],
      "metadata": {
        "id": "I5It5_8O0z8Y"
      }
    }
  ]
}