{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyORz4gT0CdfDWvqgNTIgc4O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/IntroToDNNwKeras/blob/master/Over_and_Underfitting_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we generate some random data and split it into training and test sets. We then train a linear regression model on the training set and evaluate its performance on both the training and test sets.\n",
        "\n",
        "If the training set MSE is much lower than the test set MSE, it indicates that the model has overfit the training data, i.e., it has learned the noise in the training data instead of the underlying pattern. This leads to high variance and low bias.\n",
        "\n",
        "On the other hand, if the training set MSE and test set MSE are both high, it indicates that the model has underfit the data, i.e., it is too simple to capture the underlying pattern. This leads to high bias and low variance.\n",
        "\n",
        "A good balance between bias and variance can be achieved by choosing an appropriate model complexity and regularization."
      ],
      "metadata": {
        "id": "JCPY6OoKwujB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXJFK9Z1we2o"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate some random data\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(100, 1)\n",
        "y = 2 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a linear regression model on the training set\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the training set and calculate the mean squared error\n",
        "y_train_pred = model.predict(X_train)\n",
        "mse_train = mean_squared_error(y_train, y_train_pred)\n",
        "\n",
        "# Predict on the test set and calculate the mean squared error\n",
        "y_test_pred = model.predict(X_test)\n",
        "mse_test = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "print(f\"Training set MSE: {mse_train:.2f}\")\n",
        "print(f\"Test set MSE: {mse_test:.2f}\")"
      ]
    }
  ]
}