{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNUPHrn4Cz6HrDQddeb2rB9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/IntroToDNNwKeras/blob/master/ML_Optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Keras optimizer ensures that appropriate weights and loss functions are used to keep the difference between the predicted and actual value of the neural network learning model optimized. There are various types of Keras optimizers available to choose from."
      ],
      "metadata": {
        "id": "vAa_KRNUn32Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizers are the general concept used in neural networks because it involves randomly initializing and manipulating the value of weights for every epoch to increase the model network’s accuracy potential. A comparison is made in every epoch between the output from the training data and the actual data, which helps us calculate the errors and find out the loss functions and further updation of the corresponding weights.\n",
        "\n",
        "There needs to be some way to conclude how the weight should be manipulated to get the most accuracy for which Keras optimizers come into the picture. Keras optimizer helps us achieve the ideal weights and get a loss function that is completely optimized. One of the most popular of all optimizers is gradient descent. Various other keras optimizers are available and used widely for different practical purposes. There is a provision of various"
      ],
      "metadata": {
        "id": "kdGpRoGcnUM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plot-model"
      ],
      "metadata": {
        "id": "JxmA2MhwpgU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "-RjMrT1zGOpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "metadata": {
        "id": "uKpYXxvwGg4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0X7Bh-UFu-b"
      },
      "outputs": [],
      "source": [
        "def build_compile(optimizer_name='SGD'):\n",
        "\n",
        "    # Use the same network topology as last week\n",
        "    model = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)),\n",
        "                          keras.layers.Dense(128, activation='relu'),\n",
        "                          keras.layers.Dense(10, activation='softmax')])\n",
        "\n",
        "    # compile the model with a cross-entropy loss and specify the given optimizer\n",
        "    model.compile(optimizer=optimizer_name, loss=keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "ogKJ9l1ioGKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from plot_model import plot_model\n",
        "plot_model()"
      ],
      "metadata": {
        "id": "ch-SM9t4oIYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here are various types of Keras optimizers that are listed below –\n",
        "\n",
        "Adagrad: This optimizer of Keras uses specific parameters in the learning rates. It has got its base of the frequencies made in the updates by the value of parameters, and accordingly, the working happens. The individual features affect the learning rate and are adjusted accordingly. There is also the scenario where various values of the learning rate for some weights correspond.\n",
        "Adam: This optimizer stands for Adaptive Moment estimation. This makes the adam algorithm; the gradient descent method is upgraded for the optimization tasks. It requires less memory and is very efficient. This method must go in this scenario when we have a lot of data in bulk quantity and parameters associated with it. It is most popular among developers of neural networks.\n",
        "Nadam: This optimizer makes use of the Nadam algorithm. I stand for Nesterov and adam optimizer, and the component of Nesterov is more efficient than the previous implementations. Nesterov component is used for the updation of the gradient by the Nadam optimizer.\n",
        "Adamax: It is the adaption of the algorithm of Adam optimizer hence the name Adam max. The base of this algorithm is the infinity norm. When using the models that have embeddings, it is considered superior to Adam optimizer in some scenarios.\n",
        "RMSprop: It stands for Root mean Square propagation. The main motive of the RMSprop is to make sure that there is a constant movement in the average calculation of the square of gradients, and the performance of the task of division for gradient upon the root of average also takes place."
      ],
      "metadata": {
        "id": "kVEdzR9pne43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_names = ['SGD','Momentum','Nesterov', 'RMSprop','Adagrad','Adam','NAdam']\n",
        "optimizer_list = ['SGD',keras.optimizers.SGD(learning_rate=0.01, momentum=0.5, nesterov=False),keras.optimizers.SGD(learning_rate=0.01, momentum=0.5, nesterov=True), 'RMSprop','Adagrad','Adam','NAdam']"
      ],
      "metadata": {
        "id": "b8-Fa3dWF02Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Two arrays for training and validation performance\n",
        "hist_acc = []\n",
        "hist_val_acc = []\n",
        "\n",
        "# Iterate over optimizers and train the network, using x_test and y_test as a validation set in each epoch\n",
        "for item,name in zip(optimizer_list, optimizer_names):\n",
        "    print(\"-----------------------------\")\n",
        "    print(\"Doing %s optimizer\" %str(name))\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "    # Get the model from our function above\n",
        "    model = build_compile(item)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "    # Store the performance\n",
        "    hist_acc.append(history.history['val_accuracy'])\n",
        "    hist_val_acc.append(history.history['val_accuracy'])\n",
        "    print(\"-----------------------------\")"
      ],
      "metadata": {
        "id": "ukSb8NGYGDRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy on training set\n",
        "for i in range(len(optimizer_list)):\n",
        "    plt.plot(hist_acc[i],'-o',label=str(optimizer_names[i]))\n",
        "plt.title('model accuracy on train')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8_1PPYHzGNe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy on test set\n",
        "for i in range(len(optimizer_list)):\n",
        "    plt.plot(hist_val_acc[i],'-o', label=str(optimizer_names[i]))\n",
        "plt.title('model accuracy on test')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-Ia9yI3PGwqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nepochs = 50"
      ],
      "metadata": {
        "id": "cJULe2hDG08f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement formula (15)\n",
        "initial_learning_rate = 0.01\n",
        "epochs = nepochs\n",
        "decay = initial_learning_rate / epochs\n",
        "\n",
        "def lr_time_based_decay(epoch, lr):\n",
        "    return initial_learning_rate * 1 / (1 + decay * epoch)\n"
      ],
      "metadata": {
        "id": "RXHG2dKYG3B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the learning rate as a function of the number of epochs\n",
        "plt.plot(lr_time_based_decay(np.arange(0,nepochs),0.01))\n",
        "plt.ylabel('learning rate')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hIYd6pqwG6Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the network with the learning rate schedule\n",
        "model = build_compile()\n",
        "history_time_based_decay = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=nepochs,\n",
        "    batch_size=32,\n",
        "    callbacks=[keras.callbacks.LearningRateScheduler(lr_time_based_decay, verbose=1)], validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "id": "9QVNZOXmG-Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement formula (16)\n",
        "initial_learning_rate = 0.01\n",
        "def lr_step_decay(epoch, lr):\n",
        "    drop_rate = 0.5\n",
        "    epochs_drop = 10.0\n",
        "    return initial_learning_rate * np.power(drop_rate, np.floor(epoch/epochs_drop))"
      ],
      "metadata": {
        "id": "guMNuf8HHL7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the learning rate as a function of the number of epochs\n",
        "plt.plot(lr_step_decay(np.arange(0,nepochs),0.01))\n",
        "plt.ylabel('learning rate')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lfyUTzgyHN99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the network with the learning rate schedule\n",
        "model = build_compile()\n",
        "history_step_decay = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=nepochs,\n",
        "    batch_size=32,\n",
        "    callbacks=[keras.callbacks.LearningRateScheduler(lr_step_decay, verbose=1)], validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "hf3nlkPHHQdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement formula (17)\n",
        "initial_learning_rate = 0.01\n",
        "def lr_exp_decay(epoch, lr):\n",
        "    k = 0.1\n",
        "    return initial_learning_rate * np.exp(-k*epoch)\n"
      ],
      "metadata": {
        "id": "YMqJ2PhVHWZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the learning rate as a function of the number of epochs\n",
        "plt.plot(lr_exp_decay(np.arange(0,nepochs),0.01))\n",
        "plt.ylabel('learning rate')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gkJRZ9r8HYwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the network with the learning rate schedule\n",
        "model = build_compile()\n",
        "history_exp_decay = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=nepochs,\n",
        "    batch_size=32,\n",
        "    callbacks=[keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=1)], validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "-iuo1QlgHbEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hist_acc[0],'-o',label='Constant')\n",
        "plt.plot(history_exp_decay.history['val_accuracy'],'-o', label=\"Exp. Decay\")\n",
        "plt.plot(history_step_decay.history['val_accuracy'],'-o', label=\"Step Decay\")\n",
        "plt.plot(history_time_based_decay.history['val_accuracy'],'-o', label=\"Time Decay\")\n",
        "plt.title('model accuracy on train')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZNO2EQX2Hdqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist_val_acc[0],'-o',label='Constant')\n",
        "plt.plot(history_exp_decay.history['val_accuracy'],'-o', label=\"Exp. Decay\")\n",
        "plt.plot(history_step_decay.history['val_accuracy'],'-o', label=\"Step Decay\")\n",
        "plt.plot(history_time_based_decay.history['val_accuracy'],'-o', label=\"Time Decay\")\n",
        "plt.title('model accuracy on test')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D8RbQ6YYHgP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with an L2 regularization added to all weights\n",
        "\n",
        "model_l2 = keras.Sequential([keras.layers.Flatten(input_shape=(28, 28)),\n",
        "                      keras.layers.Dense(128, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "                      keras.layers.Dense(10, activation='softmax',kernel_regularizer=keras.regularizers.l2(0.001))])\n",
        "\n",
        "# Compile the model and optimize with adam\n",
        "model_l2.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PgAyvvnFHpJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the data while providing a validation set for each epoch\n",
        "history_l2 = model_l2.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "IkuETrlUHrzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with early stopping\n",
        "model_es = keras.Sequential([keras.layers.Flatten(input_shape=(28, 28)),\n",
        "                      keras.layers.Dense(128, activation='relu'),\n",
        "                      keras.layers.Dense(10, activation='softmax')])\n",
        "\n",
        "# Compile the model and optimize with adam\n",
        "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "model_es.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "N5bFbZa1HueH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the data while providing a validation set for each epoch\n",
        "history_es = model_es.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test), callbacks=[es])\n"
      ],
      "metadata": {
        "id": "nCT7qjKjHxOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with dropout\n",
        "model_dropout = keras.Sequential([keras.layers.Flatten(input_shape=(28, 28)),\n",
        "                      keras.layers.Dense(128, activation='relu'),\n",
        "                      keras.layers.Dropout(.2),\n",
        "                      keras.layers.Dense(10, activation='softmax'),\n",
        "                      keras.layers.Dropout(.2)])\n",
        "\n",
        "# Compile the model and optimize with adam\n",
        "model_dropout.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "BwUhMBZ7H0OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the data while providing a validation set for each epoch\n",
        "history_dropout = model_dropout.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "id": "mSOERrNKH2Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hist_val_acc[0],'-o', label='Standard')\n",
        "plt.plot(history_l2.history['val_acc'],'-o', label=\"L2\")\n",
        "plt.plot(history_es.history['val_acc'],'-o', label=\"Early Stopping\")\n",
        "plt.plot(history_dropout.history['val_acc'],'-o', label=\"Dropout\")\n",
        "plt.title('model accuracy on test')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YbzOQx-LH4sV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}