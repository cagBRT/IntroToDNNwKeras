{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNVXc8jMMIJy7bsJ2KT0efX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/IntroToDNNwKeras/blob/master/ML_Optimizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "-RjMrT1zGOpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "metadata": {
        "id": "uKpYXxvwGg4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0X7Bh-UFu-b"
      },
      "outputs": [],
      "source": [
        "def build_compile(optimizer_name='SGD'):\n",
        "\n",
        "    # Use the same network topology as last week\n",
        "    model = keras.Sequential([ keras.layers.Flatten(input_shape=(28, 28)),\n",
        "                          keras.layers.Dense(128, activation='relu'),\n",
        "                          keras.layers.Dense(10, activation='softmax')])\n",
        "\n",
        "    # compile the model with a cross-entropy loss and specify the given optimizer\n",
        "    model.compile(optimizer=optimizer_name, loss=keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer_names = ['SGD','Momentum','Nesterov', 'RMSprop','Adagrad','Adam','NAdam']\n",
        "optimizer_list = ['SGD',keras.optimizers.SGD(learning_rate=0.01, momentum=0.5, nesterov=False),keras.optimizers.SGD(learning_rate=0.01, momentum=0.5, nesterov=True), 'RMSprop','Adagrad','Adam','NAdam']"
      ],
      "metadata": {
        "id": "b8-Fa3dWF02Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Two arrays for training and validation performance\n",
        "hist_acc = []\n",
        "hist_val_acc = []\n",
        "\n",
        "# Iterate over optimizers and train the network, using x_test and y_test as a validation set in each epoch\n",
        "for item,name in zip(optimizer_list, optimizer_names):\n",
        "    print(\"-----------------------------\")\n",
        "    print(\"Doing %s optimizer\" %str(name))\n",
        "    print(\"-----------------------------\")\n",
        "\n",
        "    # Get the model from our function above\n",
        "    model = build_compile(item)\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))\n",
        "\n",
        "    # Store the performance\n",
        "    hist_acc.append(history.history['val_accuracy'])\n",
        "    hist_val_acc.append(history.history['val_accuracy'])\n",
        "    print(\"-----------------------------\")"
      ],
      "metadata": {
        "id": "ukSb8NGYGDRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy on training set\n",
        "for i in range(len(optimizer_list)):\n",
        "    plt.plot(hist_acc[i],'-o',label=str(optimizer_names[i]))\n",
        "plt.title('model accuracy on train')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8_1PPYHzGNe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy on test set\n",
        "for i in range(len(optimizer_list)):\n",
        "    plt.plot(hist_val_acc[i],'-o', label=str(optimizer_names[i]))\n",
        "plt.title('model accuracy on test')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-Ia9yI3PGwqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nepochs = 50"
      ],
      "metadata": {
        "id": "cJULe2hDG08f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement formula (15)\n",
        "initial_learning_rate = 0.01\n",
        "epochs = nepochs\n",
        "decay = initial_learning_rate / epochs\n",
        "\n",
        "def lr_time_based_decay(epoch, lr):\n",
        "    return initial_learning_rate * 1 / (1 + decay * epoch)\n"
      ],
      "metadata": {
        "id": "RXHG2dKYG3B1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the learning rate as a function of the number of epochs\n",
        "plt.plot(lr_time_based_decay(np.arange(0,nepochs),0.01))\n",
        "plt.ylabel('learning rate')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hIYd6pqwG6Hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the network with the learning rate schedule\n",
        "model = build_compile()\n",
        "history_time_based_decay = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=nepochs,\n",
        "    batch_size=32,\n",
        "    callbacks=[keras.callbacks.LearningRateScheduler(lr_time_based_decay, verbose=1)], validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "id": "9QVNZOXmG-Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement formula (16)\n",
        "initial_learning_rate = 0.01\n",
        "def lr_step_decay(epoch, lr):\n",
        "    drop_rate = 0.5\n",
        "    epochs_drop = 10.0\n",
        "    return initial_learning_rate * np.power(drop_rate, np.floor(epoch/epochs_drop))"
      ],
      "metadata": {
        "id": "guMNuf8HHL7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the learning rate as a function of the number of epochs\n",
        "plt.plot(lr_step_decay(np.arange(0,nepochs),0.01))\n",
        "plt.ylabel('learning rate')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lfyUTzgyHN99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the network with the learning rate schedule\n",
        "model = build_compile()\n",
        "history_step_decay = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=nepochs,\n",
        "    batch_size=32,\n",
        "    callbacks=[keras.callbacks.LearningRateScheduler(lr_step_decay, verbose=1)], validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "hf3nlkPHHQdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement formula (17)\n",
        "initial_learning_rate = 0.01\n",
        "def lr_exp_decay(epoch, lr):\n",
        "    k = 0.1\n",
        "    return initial_learning_rate * np.exp(-k*epoch)\n"
      ],
      "metadata": {
        "id": "YMqJ2PhVHWZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the learning rate as a function of the number of epochs\n",
        "plt.plot(lr_exp_decay(np.arange(0,nepochs),0.01))\n",
        "plt.ylabel('learning rate')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gkJRZ9r8HYwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the network with the learning rate schedule\n",
        "model = build_compile()\n",
        "history_exp_decay = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=nepochs,\n",
        "    batch_size=32,\n",
        "    callbacks=[keras.callbacks.LearningRateScheduler(lr_exp_decay, verbose=1)], validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "-iuo1QlgHbEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hist_acc[0],'-o',label='Constant')\n",
        "plt.plot(history_exp_decay.history['val_accuracy'],'-o', label=\"Exp. Decay\")\n",
        "plt.plot(history_step_decay.history['val_accuracy'],'-o', label=\"Step Decay\")\n",
        "plt.plot(history_time_based_decay.history['val_accuracy'],'-o', label=\"Time Decay\")\n",
        "plt.title('model accuracy on train')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZNO2EQX2Hdqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist_val_acc[0],'-o',label='Constant')\n",
        "plt.plot(history_exp_decay.history['val_accuracy'],'-o', label=\"Exp. Decay\")\n",
        "plt.plot(history_step_decay.history['val_accuracy'],'-o', label=\"Step Decay\")\n",
        "plt.plot(history_time_based_decay.history['val_accuracy'],'-o', label=\"Time Decay\")\n",
        "plt.title('model accuracy on test')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D8RbQ6YYHgP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with an L2 regularization added to all weights\n",
        "\n",
        "model_l2 = keras.Sequential([keras.layers.Flatten(input_shape=(28, 28)),\n",
        "                      keras.layers.Dense(128, activation='relu',kernel_regularizer=keras.regularizers.l2(0.001)),\n",
        "                      keras.layers.Dense(10, activation='softmax',kernel_regularizer=keras.regularizers.l2(0.001))])\n",
        "\n",
        "# Compile the model and optimize with adam\n",
        "model_l2.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "PgAyvvnFHpJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the data while providing a validation set for each epoch\n",
        "history_l2 = model_l2.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "IkuETrlUHrzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with early stopping\n",
        "model_es = keras.Sequential([keras.layers.Flatten(input_shape=(28, 28)),\n",
        "                      keras.layers.Dense(128, activation='relu'),\n",
        "                      keras.layers.Dense(10, activation='softmax')])\n",
        "\n",
        "# Compile the model and optimize with adam\n",
        "es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "model_es.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "N5bFbZa1HueH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the data while providing a validation set for each epoch\n",
        "history_es = model_es.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test), callbacks=[es])\n"
      ],
      "metadata": {
        "id": "nCT7qjKjHxOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model with dropout\n",
        "model_dropout = keras.Sequential([keras.layers.Flatten(input_shape=(28, 28)),\n",
        "                      keras.layers.Dense(128, activation='relu'),\n",
        "                      keras.layers.Dropout(.2),\n",
        "                      keras.layers.Dense(10, activation='softmax'),\n",
        "                      keras.layers.Dropout(.2)])\n",
        "\n",
        "# Compile the model and optimize with adam\n",
        "model_dropout.compile(optimizer='Adam', loss=keras.losses.SparseCategoricalCrossentropy(),metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "BwUhMBZ7H0OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the model to the data while providing a validation set for each epoch\n",
        "history_dropout = model_dropout.fit(x_train, y_train, epochs=50, batch_size=32, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "id": "mSOERrNKH2Oy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(hist_val_acc[0],'-o', label='Standard')\n",
        "plt.plot(history_l2.history['val_acc'],'-o', label=\"L2\")\n",
        "plt.plot(history_es.history['val_acc'],'-o', label=\"Early Stopping\")\n",
        "plt.plot(history_dropout.history['val_acc'],'-o', label=\"Dropout\")\n",
        "plt.title('model accuracy on test')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YbzOQx-LH4sV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}