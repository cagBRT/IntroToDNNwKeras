{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Choosing Loss Functions: Multi Classif.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyORg/6lOvbWwAwDROdsg2tO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cagBRT/IntroToDNNwKeras/blob/master/Choosing_Loss_Functions_Multi_Classif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjIGzxKa6fQn"
      },
      "source": [
        "# **Multi-Class Classification Loss Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRXc98UK6wUO"
      },
      "source": [
        "This notebook looks at loss fuctions for problems where examples are assigned to one of three or more classes. <br>\n",
        "For example: Image classification of the MNIST dataset, it has 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nppkzowW7Ess"
      },
      "source": [
        "**Install and import required libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIDz1Cv2UdYH"
      },
      "source": [
        "#!pip install keras.utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7IyvB6GUTsc"
      },
      "source": [
        "# mlp for the blobs multi-class classification problem with cross-entropy loss\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import utils\n",
        "from sklearn.datasets import make_blobs\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from numpy import where\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqqU9h_-79cm"
      },
      "source": [
        "Create a synthetic dataset<br>\n",
        "1000 examples, 3 classes, 2 features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyCLIf93T4OA"
      },
      "source": [
        "# generate dataset\n",
        "dataX, y = make_blobs(n_samples=2000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        "#y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiA8nUgp8NgT"
      },
      "source": [
        "Plot the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAPCdShBV9ml"
      },
      "source": [
        "# select indices of points with each class label\n",
        "for i in range(3):\n",
        "\tsamples_ix = where(y == i)\n",
        "\tpyplot.scatter(dataX[samples_ix, 0], dataX[samples_ix, 1])\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29cOo3-7zQx-"
      },
      "source": [
        "**Standardize the data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeBaWyDAxI83"
      },
      "source": [
        "scaler = StandardScaler()\n",
        "X=scaler.fit_transform(dataX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSuLiGYbztZB"
      },
      "source": [
        "## **Cross-Entropy Loss**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD089wkH8TOO"
      },
      "source": [
        "**Split the data into training and test sets**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CsBQaP4z_vw"
      },
      "source": [
        "Note for Cross-Entropy we have to convert the labels to one-hot encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4UpcfK75AAI"
      },
      "source": [
        "The three outputs:<br>\n",
        "\n",
        "(10 10 01) = 2<br>\n",
        "(10 01 10) = 1<br>\n",
        "(01 10 10) = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMtrbh0FujPA"
      },
      "source": [
        "y = tf.keras.utils.to_categorical(y)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjJD02MVT_wA"
      },
      "source": [
        "# split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY9V-LoxuZOD"
      },
      "source": [
        "# The patience parameter is the amount of epochs to check for improvement\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='loss', patience=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQ7Gqn48wwC8"
      },
      "source": [
        "**Define the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYLuS0sr3GnX"
      },
      "source": [
        "Two inputs and three outputs and one internal dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtPES9g9UEAe"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsGXHtW-nPEN"
      },
      "source": [
        "# The patience parameter is the amount of epochs to check for improvement\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='loss', patience=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-U8DrABxzymJ"
      },
      "source": [
        "Cross-entropy is the default loss function to use for multi-class classification models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nih4NZ-pw1pL"
      },
      "source": [
        "**Train the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTTl3wWOWUh6"
      },
      "source": [
        "# compile model\n",
        "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "# fit model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0,callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u05VE5rGw7Tz"
      },
      "source": [
        "**Evaluate the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hci0QsOWdYb"
      },
      "source": [
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve_gBzC3Whmr"
      },
      "source": [
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "# plot loss during training\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Loss')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "# plot accuracy during training\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.plot(history.history['accuracy'], label='train')\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6faoZQ00sXn"
      },
      "source": [
        "**Assignment**:<br>\n",
        "Run the same code, but this time don't standardize the input data. \n",
        "Does this change the accuracy? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7zZj3uZXgYy"
      },
      "source": [
        "## **Sparse Multiclass Cross-Entropy Loss**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE-c4q-50U-l"
      },
      "source": [
        "Cross entropy requires converting labels to one-hot encoding.<br>\n",
        "Which can lead to a sparse label matrix.\n",
        "It can also lead to a larger number of inputs on the model, which means a larger model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRdGaBc_33f1"
      },
      "source": [
        "Sparse MultiClass Cross-Entropy loss does not require one-hot encoding. <br>\n",
        "Notice we don't do the categorical conversion for this dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5oRCtZl3TK-"
      },
      "source": [
        "**Create the dataset and split into test and training sets**<br>\n",
        "This is the same dataset as used in the example above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyFEE2OpXlY4"
      },
      "source": [
        "# generate a classification dataset\n",
        "X, y = make_blobs(n_samples=2000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        "# split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-YV5iy42mou"
      },
      "source": [
        "# select indices of points with each class label\n",
        "for i in range(3):\n",
        "\tsamples_ix = where(y == i)\n",
        "\tpyplot.scatter(X[samples_ix, 0], X[samples_ix, 1])\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4UF-XwwXzUT"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwAYG6tqX2bC"
      },
      "source": [
        "# compile model\n",
        "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5YUEczxX8wG"
      },
      "source": [
        "# fit model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0 ,callbacks=[early_stop])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdGYEZFlYNnO"
      },
      "source": [
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrYgiCEtYR3B"
      },
      "source": [
        "# plot loss during training\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Loss')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "# plot accuracy during training\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.plot(history.history['accuracy'], label='train')\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBIUAfX7ZROn"
      },
      "source": [
        "**Kullback Leibler Divergence Loss**<br>\n",
        "\n",
        "KL Divergence measures how one probability differs from a baseline distribution<br>\n",
        "KL Divergence is usually used for more complex functions, such as autoencoders.<br>But it can be used for Multi-Class Classification, where it is functionally equivalent to multi-class cross entropy. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t37sRyCHZYF_"
      },
      "source": [
        "# generate 2d classification dataset\n",
        "X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n",
        "# one hot encode output variable\n",
        "y = tf.keras.utils.to_categorical(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWX1K-cm56v4"
      },
      "source": [
        "# split into train and test\n",
        "n_train = 500\n",
        "X_train, X_test = X[:n_train, :], X[n_train:, :]\n",
        "y_train, y_test = y[:n_train], y[n_train:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ofy-nm1059_-"
      },
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n",
        "model.add(Dense(3, activation='softmax'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCzMIXLT6Bzi"
      },
      "source": [
        "# compile model\n",
        "opt = SGD(learning_rate=0.01, momentum=0.9)\n",
        "model.compile(loss='kullback_leibler_divergence', optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRtJWPx36I0n"
      },
      "source": [
        "# fit model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PIkWJQT6M-o"
      },
      "source": [
        "# evaluate the model\n",
        "_, train_acc = model.evaluate(X_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMNI4XOr6P3m"
      },
      "source": [
        "# plot loss during training\n",
        "pyplot.subplot(211)\n",
        "pyplot.title('Loss')\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "# plot accuracy during training\n",
        "pyplot.subplot(212)\n",
        "pyplot.title('Accuracy')\n",
        "pyplot.plot(history.history['accuracy'], label='train')\n",
        "pyplot.plot(history.history['val_accuracy'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}